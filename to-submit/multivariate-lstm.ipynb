{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('python version', sys.version_info)\n",
    "print('tf version', tf.__version__, 'keras version', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Statlog Shuttle multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes after labelling outliers (array([1, 2, 3, 5, 6, 7]), array([45586,    50,   171,  3267,    10,    13]))\n",
      "Unique outliers after labelling outliers (array([0, 1]), array([45586,  3511]))\n",
      "Look back data shapes: lookbackX (49073, 24, 8) lookbackY (49073, 9)\n",
      "Shapes: Xtrain_full (39258, 24, 8), Ytrain_full (39258, 8), Xtest (9815, 24, 8), Ytest (9815, 8)\n",
      "Shapes: Xtrain (35333, 24, 8), Ytrain (35333, 8), Xvalid (3925, 24, 8), Yvalid (3925, 8)\n",
      "369/369 [==============================] - 5s 15ms/step - loss: 0.0023\n",
      "369/369 [==============================] - 4s 12ms/step - loss: 7.4962e-04\n",
      "369/369 [==============================] - 5s 15ms/step - loss: 0.0013\n",
      "Time to train 736069\n",
      "Best parameters {'n_units': 5, 'n_hidden': 1} best score 0.0014293645896638434:\n",
      "Train Score: 0.00129 MSE 0.03593 RMSE\n",
      "Test Score: 0.00728 MSE 0.08530 RMSE\n",
      "Saving model model-shuttle-lstm/shuttle-lstm.h5\n",
      "Deviations Mins [1.19780280e-06 1.78813934e-07 1.89209647e-07 9.78127504e-08\n",
      " 4.35774063e-07 2.04993229e-06 1.26384809e-07 2.52984344e-07], Maxes [0.58820552 0.5101077  0.50917581 0.66136318 0.56898034 0.61230803\n",
      " 0.50706047 0.52676505]\n",
      "Deviations 99.5th pctiles [0.09859156 0.199896   0.04492987 0.10863996 0.09254799 0.24272896\n",
      " 0.27313398 0.24550381]\n",
      "Shape of deviations above threshold matrix (49073, 8)\n",
      "Any feature deviation > its 99.5th pctile deviation based is_anomaly labels (array([0., 1.]), array([47614,  1459]))\n",
      "Shape of predicted labels (49073, 1)\n",
      "Actual is_anomaly labels in data (array([0, 1]), array([45562,  3511]))\n",
      "Confusion matrix \n",
      "[[44908   654]\n",
      " [ 2706   805]]\n",
      "precision 0.55175, recall 0.22928, f1 0.32394\n"
     ]
    }
   ],
   "source": [
    "# %%writefile Shuttle-LSTM.py\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "current_time_millis = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "\"\"\"\n",
    "Shuttle data set specific data prep\n",
    "\n",
    "\"\"\"\n",
    "def label_outliers(nasa_df_row):\n",
    "    if nasa_df_row['class'] == 1 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "\"\"\"\n",
    "Shuttle data set specific data prep\n",
    "\n",
    "\"\"\"\n",
    "def cleanup() :\n",
    "    colnames =['time', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'class']\n",
    "    train_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.trn/shuttle.trn\",names=colnames,sep=\" \")\n",
    "    test_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.tst\",names=colnames,sep=\" \")\n",
    "\n",
    "    # merge train and test\n",
    "    merged_df = pd.concat([train_df, test_df])\n",
    "    # print(\"Unique classes {}\".format(np.unique(merged_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # drop class = 4\n",
    "    minus4_df = merged_df.loc[merged_df['class'] != 4]\n",
    "    # print(\"Frame after dropping 4 \\n{}\".format(minus4_df))\n",
    "    # print(\"Unique classes after dropping 4 {}\".format(np.unique(minus4_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # mark class 1 as inlier and rest as outlier\n",
    "    is_anomaly_column = minus4_df.apply(lambda row: label_outliers(row), axis=1)\n",
    "    labelled_df = minus4_df.assign(is_anomaly=is_anomaly_column.values)\n",
    "\n",
    "    #print(\"Frame after labelling outliers \\n{}\".format(labelled_df))\n",
    "    print(\"Unique classes after labelling outliers {}\".format(np.unique(labelled_df['class'].values, return_counts=True)))\n",
    "    print(\"Unique outliers after labelling outliers {}\".format(np.unique(labelled_df['is_anomaly'].values, return_counts=True)))\n",
    "\n",
    "    # sort by time\n",
    "\n",
    "    sorted_df = labelled_df.sort_values('time')\n",
    "\n",
    "    #print(\"Sorted Frame\\n{}\".format(sorted_df))\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "\"\"\"\n",
    "General data read from pandas data frame\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def read_data_with_labels(df, timeVariantColumns, labelColumnNum):\n",
    "#     df = pd.read_csv(file)\n",
    "    data = df.values.astype('float64')\n",
    "    tsData = df[timeVariantColumns].values.astype('float64')\n",
    "    labels = data[:, labelColumnNum].reshape((-1,1))\n",
    "    tsDataWithLabels = np.hstack((tsData, labels))\n",
    "    return tsDataWithLabels, data\n",
    "\n",
    "\"\"\"\n",
    "General data feature scaling\n",
    "\n",
    "\"\"\"\n",
    "def scale(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(data)\n",
    "    return scaler, scaler.transform(data)\n",
    "\n",
    "\"\"\"\n",
    "General look back data preparation for model input.\n",
    "\n",
    "Input expected to be a 2D numpy array with last column being label\n",
    "Returns looked back X and Y; last column in look back Y data returned is label\n",
    "Only one step ahead prediction setting is expected.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)\n",
    "\n",
    "\"\"\"\n",
    "General train/test data split \n",
    "Input: whole numpy array input dataset (lookedback)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def split_data_set(dataset, split=0.67):\n",
    "    train_size = int(len(dataset) * split)\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    return train, test\n",
    "\n",
    "\"\"\"\n",
    "General train/validate data split \n",
    "\n",
    "Input: Xtrain, Ytrain numpy array of full train data\n",
    "\n",
    "\"\"\"\n",
    "def get_train_validation(Xtrain, Ytrain, validation_ratio=0.1):\n",
    "    validation_size = int(len(Xtrain) * validation_ratio)\n",
    "    Xtrain, Xvalid = Xtrain[validation_size:], Xtrain[:validation_size]\n",
    "    Ytrain, Yvalid = Ytrain[validation_size:], Ytrain[:validation_size]\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid\n",
    "\n",
    "\"\"\"\n",
    "This is general baseline model passed to KerasRegressor for model search.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def baseline_model(input_shape, learning_rate):\n",
    "    def build_model(input_shape=input_shape, n_hidden = 1, n_units = 50, learning_rate = learning_rate):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        for layer in range(n_hidden - 1):\n",
    "            # return sequence = true for all layers except last layer\n",
    "            model.add(keras.layers.LSTM(n_units, return_sequences = True, activation = 'relu'))\n",
    "        model.add(keras.layers.LSTM(n_units, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(input_shape[1]))\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "General function to get prediction errors/deviations of all features\n",
    "\n",
    "\"\"\"\n",
    "def get_deviations_mv(model, X, Y):\n",
    "    deviations = np.absolute(Y - model.predict(X))\n",
    "    print(\"Deviations Mins {}, Maxes {}\".format(np.amin(deviations, axis=0), np.amax(deviations, axis=0)))    \n",
    "    return deviations\n",
    "\n",
    "\"\"\"\n",
    "General function to get is_anomaly (0/1) labels for each record based on record's feature prediction error/deviation.\n",
    "For a record, if any feature prediction error/deviation is above the threshold percentile, then the record is anomaly. \n",
    "\n",
    "\"\"\"\n",
    "def get_records_above_deviation_pctile_mv(model, X, Y, pctile=95):\n",
    "    deviations = get_deviations_mv(model, X, Y) # n_samples x n_features\n",
    "    pctileDeviations = np.percentile(deviations, q=pctile, axis=0) # 1 x n_features\n",
    "    print(\"Deviations {}th pctiles {}\".format(pctile, pctileDeviations ))\n",
    "    \n",
    "    deviations_above_threshold = deviations > pctileDeviations # n_samples x n_features\n",
    "    print(\"Shape of deviations above threshold matrix {}\".format(deviations_above_threshold.shape))\n",
    "\n",
    "    predicted_labels = np.ndarray((deviations.shape[0], 1)) # n_samples x 1\n",
    "    predicted_labels_ref = deviations_above_threshold.any(axis = 1, out = predicted_labels, keepdims = True)\n",
    "    print(\"Any feature deviation > its {}th pctile deviation based is_anomaly labels {}\"\n",
    "          .format(pctile, np.unique(predicted_labels_ref, return_counts = True)))\n",
    "    return predicted_labels_ref\n",
    "\n",
    "\"\"\"\n",
    "General function to get standard binary classification scores i.e. anomaly performance score based on actual versus predicted\n",
    "is_anomaly labels for records.\n",
    "\n",
    "\"\"\"\n",
    "def get_classification_metrics(actual, predicted):\n",
    "    return confusion_matrix(actual, predicted), precision_score(actual, predicted), \\\n",
    "    recall_score(actual, predicted), f1_score(actual, predicted)\n",
    "\n",
    "############## main for experiments with Shuttle dataset #########################\n",
    "\n",
    "split = 0.8\n",
    "look_back = 24\n",
    "learning_rate = 0.001\n",
    "n_iter = 1\n",
    "cv = 3\n",
    "batch_size=32\n",
    "early_stop_patience=3\n",
    "epochs=10\n",
    "verbosity=0\n",
    "min_delta=0.0003\n",
    "pctile = 99.5\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": np.arange(1, 5).tolist(), # upto 4 hidden layers    \n",
    "    \"n_units\" : np.arange(1, 100).tolist(), # upto 99 units in each hiden layer.\n",
    "}\n",
    "\n",
    "\n",
    "sorted_df = cleanup()\n",
    "\n",
    "timeVariantColumns = ['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8']\n",
    "labelColumnNum = 10\n",
    "\n",
    "# read data\n",
    "tsDataWithLabels, data = read_data_with_labels(sorted_df, timeVariantColumns, labelColumnNum)\n",
    "# print(\"Shapes: time variant data array with labels {}, full data {}\".format(tsDataWithLabels.shape, data.shape))\n",
    "# print(\"Unique outliers in full data array {}\".format(np.unique(data[:, -1], return_counts=True)))\n",
    "# print(\"Unique outliers in time variant data array with labels {}\".format(np.unique(tsDataWithLabels[:, -1], \n",
    "#                                                                                    return_counts=True)))\n",
    "\n",
    "# print(tsDataWithLabels)\n",
    "\n",
    "# scale data\n",
    "scaler, tsDataScaled = scale(tsDataWithLabels)\n",
    "\n",
    "# Get look back data in the 3D array shape (n_samples, n_lookback_steps, n_features)\n",
    "lookbackX, lookbackY = look_back_and_create_dataset(tsDataScaled, look_back=look_back)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "\n",
    " # split into train/test\n",
    "Xtrain_full, Xtest = split_data_set(lookbackX, split=0.8)\n",
    "Ytrain_full, Ytest = split_data_set(lookbackY[:, :-1], split=0.8)   # exclude labels     \n",
    "\n",
    "print(\"Shapes: Xtrain_full {}, Ytrain_full {}, Xtest {}, Ytest {}\".format(Xtrain_full.shape, Ytrain_full.shape, \n",
    "                                                                          Xtest.shape, Ytest.shape))\n",
    "\n",
    "# split further full train set into train and validation set\n",
    "Xtrain, Ytrain, Xvalid, Yvalid = get_train_validation(Xtrain_full, Ytrain_full, validation_ratio=0.1)\n",
    "\n",
    "print(\"Shapes: Xtrain {}, Ytrain {}, Xvalid {}, Yvalid {}\".format(Xtrain.shape, Ytrain.shape, \n",
    "                                                                  Xvalid.shape, Yvalid.shape))\n",
    "\n",
    "\n",
    "input_shape = (Xtrain.shape[1], Xtrain.shape[2])\n",
    "regressor = keras.wrappers.scikit_learn.KerasRegressor(build_fn = baseline_model(input_shape=input_shape, \n",
    "                                                                         learning_rate=learning_rate))\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=early_stop_patience, monitor='val_loss', min_delta=0.0003, \n",
    "                                                  restore_best_weights = True)\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(regressor, param_distribs, n_iter = n_iter, cv = cv, verbose = verbosity)\n",
    "\n",
    "start_millis = current_time_millis()\n",
    "rnd_search_cv.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size, validation_data=(Xvalid, Yvalid), \n",
    "                  callbacks=[early_stopping_cb], verbose=verbosity)\n",
    "\n",
    "\n",
    "end_millis = current_time_millis()\n",
    "\n",
    "print(\"Time to train {}\".format(end_millis -start_millis))\n",
    "\n",
    "model = rnd_search_cv.best_estimator_.model\n",
    "print(\"Best parameters {} best score {}:\".format(rnd_search_cv.best_params_, -rnd_search_cv.best_score_))\n",
    "\n",
    "trainMSE = model.evaluate(Xtrain_full, Ytrain_full, verbose = verbosity)\n",
    "print(\"Train Score: {0:.5f} MSE {1:.5f} RMSE\".format(trainMSE, np.sqrt(trainMSE)))\n",
    "testMSE = model.evaluate(Xtest, Ytest, verbose = verbosity)\n",
    "print(\"Test Score: {0:.5f} MSE {1:.5f} RMSE\".format(testMSE, np.sqrt(testMSE)))\n",
    "\n",
    "modeldir = 'model-shuttle-lstm'\n",
    "if not os.path.exists(modeldir):\n",
    "    os.makedirs(modeldir)\n",
    "modelpath = modeldir + '/' + 'shuttle-lstm.h5'\n",
    "print(\"Saving model\", modelpath )\n",
    "model.save(modelpath)        \n",
    "\n",
    "# get deviations for whole dataset and id records with deviations > pctile threshold and asign an is_anomaly label\n",
    "predictedLabels = get_records_above_deviation_pctile_mv(model, lookbackX, lookbackY[:, :-1], pctile)\n",
    "\n",
    "print(\"Shape of predicted labels {}\".format(predictedLabels.shape))\n",
    "\n",
    "# actual is_anomaly labels in dataset\n",
    "actualLabels = (data[look_back:, labelColumnNum] != 0.0).astype('int')    \n",
    "print(\"Actual is_anomaly labels in data\", np.unique(actualLabels, return_counts = True))\n",
    "\n",
    "# Compare calculated labels and actual labels to find confusion matrix, precision, recall, and F1\n",
    "conf_matrix, prec, recall, f1 = get_classification_metrics(actualLabels, predictedLabels)\n",
    "print(\"Confusion matrix \\n{0}\\nprecision {1:.5f}, recall {2:.5f}, f1 {3:.5f}\".format(conf_matrix, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
