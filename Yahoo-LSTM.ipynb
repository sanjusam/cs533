{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\n",
      "tf version 2.2.0 keras version 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print('python version', sys.version_info)\n",
    "print('tf version', tf.__version__, 'keras version', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark folders and file extensions\n",
      "../Stochastic-Methods/data/yahoo/dataset/ydata-labeled-time-series-anomalies-v1_0/A1Benchmark/ .. file extensions *.csv\n",
      "../Stochastic-Methods/data/yahoo/dataset/ydata-labeled-time-series-anomalies-v1_0/A2Benchmark/ .. file extensions *.csv\n",
      "../Stochastic-Methods/data/yahoo/dataset/ydata-labeled-time-series-anomalies-v1_0/A3Benchmark/ .. file extensions *TS*.csv\n",
      "../Stochastic-Methods/data/yahoo/dataset/ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/ .. file extensions *TS*.csv\n"
     ]
    }
   ],
   "source": [
    "YAHOO_DS=\"../Stochastic-Methods/data/yahoo/dataset/ydata-labeled-time-series-anomalies-v1_0\"\n",
    "DIRS_FILE_EXTENSIONS = {'A1Benchmark' : \"*.csv\", \\\n",
    "                        'A2Benchmark' : \"*.csv\", \\\n",
    "                        'A3Benchmark' : \"*TS*.csv\", \\\n",
    "                        'A4Benchmark' : \"*TS*.csv\" }\n",
    "\n",
    "print(\"benchmark folders and file extensions\")\n",
    "for DIR, extension in DIRS_FILE_EXTENSIONS.items():\n",
    "    Benchmark_dir  = YAHOO_DS + os.path.sep + DIR + os.path.sep \n",
    "    print(\"{} .. file extensions {}\".format(Benchmark_dir, extension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_labels(file, timeVariantColumns, labelColumnNum):\n",
    "    df = pd.read_csv(file)\n",
    "    data = df.values.astype('float64')\n",
    "    tsData = df[timeVariantColumns].values.astype('float64')\n",
    "    labels = data[:, labelColumnNum].reshape((-1,1))\n",
    "    tsDataWithLabels = np.hstack((tsData, labels))\n",
    "    return tsDataWithLabels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    return scaler, scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_set(dataset, split=0.67):\n",
    "    train_size = int(len(dataset) * split)\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input expected to be a 2D array with last column being label\n",
    "# Returns looked back X (n_samples, n_steps, n_features) and Y (n_samples, 2); \n",
    "# last column in looked back Y data returned is label\n",
    "# Only one step ahead prediction setting is expected.\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation(Xtrain, Ytrain, validation_ratio=0.1):\n",
    "    validation_size = int(len(Xtrain) * validation_ratio)\n",
    "    Xtrain, Xvalid = Xtrain[validation_size:], Xtrain[:validation_size]\n",
    "    Ytrain, Yvalid = Ytrain[validation_size:], Ytrain[:validation_size]\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deviations(model, X, Y):\n",
    "    deviations = np.absolute(Y - model.predict(X))\n",
    "    print(\"Deviation Min {}, Max {}\".format(np.amin(deviations, axis=0), np.amax(deviations, axis=0)))    \n",
    "    return deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_above_deviation_pctile(model, X, Y, pctile=95):\n",
    "    deviations = get_deviations(model, X, Y)\n",
    "    pctileDeviationValue = np.percentile(deviations, q=pctile, axis=0)\n",
    "    print(\"Deviation {}th pctile {}\".format(pctile, pctileDeviationValue ))\n",
    "    labels = (deviations > pctileDeviationValue).astype('int')\n",
    "    print(\"Deviation > {}th pctile is_anomaly labels in data {}\".format(pctile, np.unique(labels, return_counts = True)))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metrics(actual, predicted):\n",
    "    return confusion_matrix(actual, predicted), precision_score(actual, predicted), \\\n",
    "    recall_score(actual, predicted), f1_score(actual, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeVariantColumns = ['value']\n",
    "labelColumnNum = 2\n",
    "look_back=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../Stochastic-Methods/data/yahoo/dataset/ydata-labeled-time-series-anomalies-v1_0/A2Benchmark/synthetic_10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: time variant data with labels (1421, 2), full data (1421, 3)\n"
     ]
    }
   ],
   "source": [
    "tsDataWithLabels, data = read_data_with_labels(file_name, timeVariantColumns, labelColumnNum)\n",
    "print(\"Shapes: time variant data with labels {}, full data {}\".format(tsDataWithLabels.shape, data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1421, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler, tsDataScaled = scale(tsDataWithLabels)\n",
    "tsDataScaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get look back data in the 3D array shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look back data shapes: lookbackX (1397, 24, 1) lookbackY (1397, 2)\n"
     ]
    }
   ],
   "source": [
    "# look back and create reshaped dataset\n",
    "lookbackX, lookbackY = look_back_and_create_dataset(tsDataScaled, look_back=look_back)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "# print(\"Look back data ... first 10 \\n\")\n",
    "# for i in range (10):\n",
    "#     print(lookbackX[i], lookbackX[i].shape, lookbackY[i], lookbackY[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1117, 24, 1), (1117, 1), (280, 24, 1), (280, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_full, Xtest = split_data_set(lookbackX, split=0.8)\n",
    "Ytrain_full, Ytest = split_data_set(lookbackY[:, :-1], split=0.8) # exclude label\n",
    "\n",
    "Xtrain_full.shape, Ytrain_full.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split full train set into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1006, 24, 1), (1006, 1), (111, 24, 1), (111, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xvalid, Yvalid = get_train_validation(Xtrain_full, Ytrain_full, validation_ratio=0.1)\n",
    "Xtrain.shape, Ytrain.shape, Xvalid.shape, Yvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (Xtrain.shape[1], Xtrain.shape[2]) # (n_steps, n_features)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here the slight change in how we stack the hidden LSTM layers - special for the last LSTM layer.\n",
    "def baseline_model(input_shape, learning_rate):\n",
    "    def build_model(input_shape=input_shape, n_hidden = 1, n_units = 50, learning_rate = learning_rate):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        for layer in range(n_hidden - 1):\n",
    "            # return sequence = true for all layers except last layer\n",
    "            model.add(keras.layers.LSTM(n_units, return_sequences = True, activation = 'relu'))\n",
    "        model.add(keras.layers.LSTM(n_units, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        return model\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    \"n_hidden\": np.arange(1, 3).tolist(), # upto 2 hidden layers\n",
    "    \"n_units\": np.arange(5,6).tolist() # 5 hidden layer units/neurons\n",
    "}\n",
    "\n",
    "n_iter = 1\n",
    "cv = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "early_stop_patience = 5\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "pctile=99.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26/26 [==============================] - 3s 128ms/step - loss: 0.1574 - val_loss: 0.0974\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 2s 96ms/step - loss: 0.0563 - val_loss: 0.0222\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 2s 90ms/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 2s 83ms/step - loss: 0.0138 - val_loss: 0.0134\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 2s 83ms/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 2s 80ms/step - loss: 0.0107 - val_loss: 0.0088\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 3s 105ms/step - loss: 0.0097 - val_loss: 0.0078\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 3s 104ms/step - loss: 0.0088 - val_loss: 0.0075\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 2s 85ms/step - loss: 0.0079 - val_loss: 0.0063\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 2s 86ms/step - loss: 0.0066 - val_loss: 0.0050\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0067\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 3s 128ms/step - loss: 0.2046 - val_loss: 0.1856\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 3s 98ms/step - loss: 0.1244 - val_loss: 0.0676\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 2s 83ms/step - loss: 0.0324 - val_loss: 0.0250\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 2s 81ms/step - loss: 0.0159 - val_loss: 0.0134\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 0.0094 - val_loss: 0.0054\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.0052 - val_loss: 0.0017\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 2s 88ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0015\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 3s 124ms/step - loss: 0.1834 - val_loss: 0.1408\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 3s 100ms/step - loss: 0.1117 - val_loss: 0.0529\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 2s 82ms/step - loss: 0.0266 - val_loss: 0.0231\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 3s 114ms/step - loss: 0.0147 - val_loss: 0.0173\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 3s 104ms/step - loss: 0.0133 - val_loss: 0.0158\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 2s 83ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 2s 81ms/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 2s 88ms/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.0095 - val_loss: 0.0085\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0100\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 3s 129ms/step - loss: 0.1694 - val_loss: 0.0644\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 3s 110ms/step - loss: 0.0326 - val_loss: 0.0165\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 2s 75ms/step - loss: 0.0188 - val_loss: 0.0155\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 2s 72ms/step - loss: 0.0176 - val_loss: 0.0145\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 2s 81ms/step - loss: 0.0153 - val_loss: 0.0126\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 2s 88ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 2s 75ms/step - loss: 0.0119 - val_loss: 0.0108\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.0088 - val_loss: 0.0044\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0042\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 3s 123ms/step - loss: 0.2064 - val_loss: 0.1923\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 3s 100ms/step - loss: 0.1750 - val_loss: 0.1590\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 2s 86ms/step - loss: 0.1289 - val_loss: 0.0944\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.0471 - val_loss: 0.0283\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 2s 92ms/step - loss: 0.0201 - val_loss: 0.0132\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.0173 - val_loss: 0.0119\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 0.0155 - val_loss: 0.0121\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 2s 90ms/step - loss: 0.0142 - val_loss: 0.0105\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 0.0124 - val_loss: 0.0085\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 0.0112 - val_loss: 0.0068\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 3s 106ms/step - loss: 0.1737 - val_loss: 0.1164\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 3s 91ms/step - loss: 0.0725 - val_loss: 0.0339\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0216 - val_loss: 0.0193\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 3s 91ms/step - loss: 0.0106 - val_loss: 0.0060\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 0.0060 - val_loss: 0.0037\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0054 - val_loss: 0.0033\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.0046 - val_loss: 0.0027\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 3s 94ms/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 3s 97ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Best parameters {'n_units': 5, 'n_hidden': 2} best score 0.0069167348323389884:\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0021\n",
      "Train Score: 0.00210 MSE 0.04583 RMSE\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0018\n",
      "Test Score: 0.00180 MSE 0.04239 RMSE\n",
      "Deviation Min [9.76080236e-05], Max [0.25483835]\n",
      "Deviation 99.5th pctile [0.13183259]\n",
      "Deviation > 99.5th pctile is_anomaly labels in data (array([0, 1]), array([1390,    7]))\n",
      "Actual is_anomaly labels in data (array([0, 1]), array([1393,    4]))\n",
      "Confusion matrix \n",
      "[[1390    3]\n",
      " [   0    4]]\n",
      "precision 0.57143, recall 1.00000, f1 0.72727\n"
     ]
    }
   ],
   "source": [
    "regressor = keras.wrappers.scikit_learn.KerasRegressor(build_fn = baseline_model(input_shape=input_shape, \n",
    "                                                                                 learning_rate=learning_rate))\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=early_stop_patience, restore_best_weights = True)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(regressor, param_distribs, n_iter = n_iter, cv = cv, verbose = verbosity)\n",
    "\n",
    "rnd_search_cv.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size, validation_data=(Xvalid, Yvalid), \n",
    "          callbacks=[early_stopping_cb], verbose=verbosity)\n",
    "\n",
    "model = rnd_search_cv.best_estimator_.model\n",
    "print(\"Best parameters {} best score {}:\".format(rnd_search_cv.best_params_, -rnd_search_cv.best_score_))\n",
    "\n",
    "trainMSE = model.evaluate(Xtrain_full, Ytrain_full, verbose = verbosity)\n",
    "print(\"Train Score: {0:.5f} MSE {1:.5f} RMSE\".format(trainMSE, np.sqrt(trainMSE)))\n",
    "testMSE = model.evaluate(Xtest, Ytest, verbose = verbosity)\n",
    "print(\"Test Score: {0:.5f} MSE {1:.5f} RMSE\".format(testMSE, np.sqrt(testMSE)))\n",
    "\n",
    "# get deviations for whole dataset and id records with deviations > pctile threshold and asign an is_anomaly label\n",
    "predictedLabels = get_records_above_deviation_pctile(model, lookbackX, lookbackY[:, :-1], pctile)\n",
    "\n",
    "# actual is_anomaly labels in dataset\n",
    "actualLabels = (data[look_back:, labelColumnNum] != 0.0).astype('int')    \n",
    "print(\"Actual is_anomaly labels in data\", np.unique(actualLabels, return_counts = True))\n",
    "conf_matrix, prec, recall, f1 = get_classification_metrics(actualLabels, predictedLabels)\n",
    "print(\"Confusion matrix \\n{0}\\nprecision {1:.5f}, recall {2:.5f}, f1 {3:.5f}\".format(conf_matrix, prec, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
