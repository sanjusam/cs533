{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\n",
      "tf version 2.2.0 keras version 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print('python version', sys.version_info)\n",
    "print('tf version', tf.__version__, 'keras version', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get look back dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input expected to be a 2D array with last column being label\n",
    "# Returns looked back X adn Y; last column in look back Y data returned is label\n",
    "# Only one step ahead prediction setting is expected.\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_seq1 = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150])\n",
    "in_seq2 = np.array([15, 25, 35, 45, 55, 65, 75, 85, 95, 105, 115, 125, 135, 145, 155])\n",
    "out_seq = np.array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "labels = np.array([0,1,0,1,0,1,0,1,0,1,0,1,0,1,0]).reshape((-1, 1))\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "mvdataWithLabels = np.hstack((in_seq1, in_seq2, out_seq, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvdataWithLabels.shape, mvdataWithLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookbackX, lookbackY = look_back_and_create_dataset(mvdataWithLabels, look_back=5)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "print(\"Look back lookBackTsData ... \\n\")\n",
    "# for i in range (len(lookbackX)):\n",
    "#     print(lookbackX[i], lookbackX[i].shape, lookbackY[i], lookbackY[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lookbackX\n",
    "Y = lookbackY[:, :-1] # exclude label\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    print(X[i], X[i].shape, Y[i], Y[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Statlog Shuttle multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes after labelling outliers (array([1, 2, 3, 5, 6, 7]), array([45586,    50,   171,  3267,    10,    13]))\n",
      "Unique outliers after labelling outliers (array([0, 1]), array([45586,  3511]))\n",
      "Shapes: time variant data array with labels (49097, 9), full data (49097, 11)\n",
      "Unique outliers in full data array (array([0., 1.]), array([45586,  3511]))\n",
      "Unique outliers in time variant data array with labels (array([0., 1.]), array([45586,  3511]))\n",
      "Look back data shapes: lookbackX (49073, 24, 8) lookbackY (49073, 9)\n",
      "Shapes: Xtrain_full (39258, 24, 8), Ytrain_full (39258, 8), Xtest (9815, 24, 8), Ytest (9815, 8)\n",
      "Shapes: Xtrain (35333, 24, 8), Ytrain (35333, 8), Xvalid (3925, 24, 8), Yvalid (3925, 8)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] n_units=5, n_hidden=1 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "737/737 - 27s - loss: 0.0970 - val_loss: 0.0123\n",
      "Epoch 2/10\n",
      "737/737 - 28s - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 3/10\n",
      "737/737 - 27s - loss: 7.4686e-04 - val_loss: 0.0022\n",
      "Epoch 4/10\n",
      "737/737 - 27s - loss: 7.4565e-04 - val_loss: 0.0022\n",
      "Epoch 5/10\n",
      "737/737 - 27s - loss: 7.4571e-04 - val_loss: 0.0022\n",
      "369/369 [==============================] - 4s 10ms/step - loss: 0.0023\n",
      "[CV] ............................ n_units=5, n_hidden=1, total= 2.4min\n",
      "[CV] n_units=5, n_hidden=1 ...........................................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/737 - 26s - loss: 0.0237 - val_loss: 0.0020\n",
      "Epoch 2/10\n",
      "737/737 - 26s - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 3/10\n",
      "737/737 - 26s - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 4/10\n",
      "737/737 - 26s - loss: 0.0013 - val_loss: 0.0017\n",
      "369/369 [==============================] - 3s 9ms/step - loss: 8.9268e-04\n",
      "[CV] ............................ n_units=5, n_hidden=1, total= 1.8min\n",
      "[CV] n_units=5, n_hidden=1 ...........................................\n",
      "Epoch 1/10\n",
      "737/737 - 27s - loss: 0.0494 - val_loss: 0.0018\n",
      "Epoch 2/10\n",
      "737/737 - 27s - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 3/10\n",
      "737/737 - 28s - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 4/10\n",
      "737/737 - 27s - loss: 0.0012 - val_loss: 0.0016\n",
      "369/369 [==============================] - 4s 10ms/step - loss: 0.0012\n",
      "[CV] ............................ n_units=5, n_hidden=1, total= 1.9min\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1105/1105 - 40s - loss: 0.0788 - val_loss: 0.0033\n",
      "Epoch 2/10\n",
      "1105/1105 - 40s - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 3/10\n",
      "1105/1105 - 40s - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 4/10\n",
      "1105/1105 - 40s - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 5/10\n",
      "1105/1105 - 40s - loss: 0.0012 - val_loss: 0.0019\n",
      "Best parameters {'n_units': 5, 'n_hidden': 1} best score 0.0014627903001382947:\n",
      "1227/1227 - 11s - loss: 0.0012\n",
      "Train Score: 0.00124 MSE 0.03522 RMSE\n",
      "307/307 - 3s - loss: 0.0063\n",
      "Test Score: 0.00625 MSE 0.07907 RMSE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "current_time_millis = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "def label_outliers(nasa_df_row):\n",
    "    if nasa_df_row['class'] == 1 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "    \n",
    "def cleanup() :\n",
    "    colnames =['time', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'class']\n",
    "    train_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.trn/shuttle.trn\",names=colnames,sep=\" \")\n",
    "    test_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.tst\",names=colnames,sep=\" \")\n",
    "\n",
    "    # merge train and test\n",
    "    merged_df = pd.concat([train_df, test_df])\n",
    "    # print(\"Unique classes {}\".format(np.unique(merged_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # drop class = 4\n",
    "    minus4_df = merged_df.loc[merged_df['class'] != 4]\n",
    "    # print(\"Frame after dropping 4 \\n{}\".format(minus4_df))\n",
    "    # print(\"Unique classes after dropping 4 {}\".format(np.unique(minus4_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # mark class 1 as inlier and rest as outlier\n",
    "    is_anomaly_column = minus4_df.apply(lambda row: label_outliers(row), axis=1)\n",
    "    labelled_df = minus4_df.assign(is_anomaly=is_anomaly_column.values)\n",
    "\n",
    "    #print(\"Frame after labelling outliers \\n{}\".format(labelled_df))\n",
    "    print(\"Unique classes after labelling outliers {}\".format(np.unique(labelled_df['class'].values, return_counts=True)))\n",
    "    print(\"Unique outliers after labelling outliers {}\".format(np.unique(labelled_df['is_anomaly'].values, return_counts=True)))\n",
    "\n",
    "    # sort by time\n",
    "\n",
    "    sorted_df = labelled_df.sort_values('time')\n",
    "\n",
    "    #print(\"Sorted Frame\\n{}\".format(sorted_df))\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "def read_data_with_labels(df, timeVariantColumns, labelColumnNum):\n",
    "#     df = pd.read_csv(file)\n",
    "    data = df.values.astype('float64')\n",
    "    tsData = df[timeVariantColumns].values.astype('float64')\n",
    "    labels = data[:, labelColumnNum].reshape((-1,1))\n",
    "    tsDataWithLabels = np.hstack((tsData, labels))\n",
    "    return tsDataWithLabels, data\n",
    "\n",
    "def scale(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(data)\n",
    "    return scaler, scaler.transform(data)\n",
    "\n",
    "\"\"\"\n",
    "# input expected to be a 2D array with last column being label\n",
    "# Returns looked back X adn Y; last column in look back Y data returned is label\n",
    "# Only one step ahead prediction setting is expected.\n",
    "\"\"\"\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)\n",
    "\n",
    "def split_data_set(dataset, split=0.67):\n",
    "    train_size = int(len(dataset) * split)\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    return train, test\n",
    "\n",
    "def get_train_validation(Xtrain, Ytrain, validation_ratio=0.1):\n",
    "    validation_size = int(len(Xtrain) * validation_ratio)\n",
    "    Xtrain, Xvalid = Xtrain[validation_size:], Xtrain[:validation_size]\n",
    "    Ytrain, Yvalid = Ytrain[validation_size:], Ytrain[:validation_size]\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid\n",
    "\n",
    "# Note here the slight change in how we stack the hidden LSTM layers - special for the last LSTM layer.\n",
    "def baseline_model(input_shape, learning_rate):\n",
    "    def build_model(input_shape=input_shape, n_hidden = 1, n_units = 50, learning_rate = learning_rate):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        for layer in range(n_hidden - 1):\n",
    "            # return sequence = true for all layers except last layer\n",
    "            model.add(keras.layers.LSTM(n_units, return_sequences = True, activation = 'relu'))\n",
    "        model.add(keras.layers.LSTM(n_units, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(input_shape[1]))\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "    \n",
    "\n",
    "############## main #########################\n",
    "\n",
    "split = 0.8\n",
    "look_back = 24\n",
    "learning_rate = 0.001\n",
    "n_iter = 1\n",
    "cv=3\n",
    "batch_size=32\n",
    "early_stop_patience=3\n",
    "epochs=10\n",
    "verbosity=2\n",
    "min_delta=0.0003\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": np.arange(1, 2).tolist(), # upto 1 hidden layers\n",
    "    \"n_units\": np.arange(5,6).tolist() # 5 hidden layer units/neurons\n",
    "}\n",
    "\n",
    "\n",
    "sorted_df = cleanup()\n",
    "\n",
    "timeVariantColumns = ['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8']\n",
    "labelColumnNum = 10\n",
    "\n",
    "# read data\n",
    "tsDataWithLabels, data = read_data_with_labels(sorted_df, timeVariantColumns, labelColumnNum)\n",
    "print(\"Shapes: time variant data array with labels {}, full data {}\".format(tsDataWithLabels.shape, data.shape))\n",
    "print(\"Unique outliers in full data array {}\".format(np.unique(data[:, -1], return_counts=True)))\n",
    "print(\"Unique outliers in time variant data array with labels {}\".format(np.unique(tsDataWithLabels[:, -1], \n",
    "                                                                                   return_counts=True)))\n",
    "\n",
    "# print(tsDataWithLabels)\n",
    "\n",
    "# scale data\n",
    "scaler, tsDataScaled = scale(tsDataWithLabels)\n",
    "\n",
    "# Get look back data in the 3D array shape (n_samples, n_lookback_steps, n_features)\n",
    "lookbackX, lookbackY = look_back_and_create_dataset(tsDataScaled, look_back=look_back)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "\n",
    " # split into train/test\n",
    "Xtrain_full, Xtest = split_data_set(lookbackX, split=0.8)\n",
    "Ytrain_full, Ytest = split_data_set(lookbackY[:, :-1], split=0.8)   # exclude labels     \n",
    "\n",
    "print(\"Shapes: Xtrain_full {}, Ytrain_full {}, Xtest {}, Ytest {}\".format(Xtrain_full.shape, Ytrain_full.shape, \n",
    "                                                                          Xtest.shape, Ytest.shape))\n",
    "\n",
    "# split further full train set into train and validation set\n",
    "Xtrain, Ytrain, Xvalid, Yvalid = get_train_validation(Xtrain_full, Ytrain_full, validation_ratio=0.1)\n",
    "\n",
    "print(\"Shapes: Xtrain {}, Ytrain {}, Xvalid {}, Yvalid {}\".format(Xtrain.shape, Ytrain.shape, \n",
    "                                                                  Xvalid.shape, Yvalid.shape))\n",
    "\n",
    "\n",
    "input_shape = (Xtrain.shape[1], Xtrain.shape[2])\n",
    "regressor = keras.wrappers.scikit_learn.KerasRegressor(build_fn = baseline_model(input_shape=input_shape, \n",
    "                                                                         learning_rate=learning_rate))\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=early_stop_patience, monitor='val_loss', min_delta=0.0003, \n",
    "                                                  restore_best_weights = True)\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(regressor, param_distribs, n_iter = n_iter, cv = cv, verbose = verbosity)\n",
    "\n",
    "start_millis = current_time_millis()\n",
    "rnd_search_cv.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size, validation_data=(Xvalid, Yvalid), \n",
    "                  callbacks=[early_stopping_cb], verbose=verbosity)\n",
    "\n",
    "\n",
    "end_millis = current_time_millis()\n",
    "\n",
    "model = rnd_search_cv.best_estimator_.model\n",
    "print(\"Best parameters {} best score {}:\".format(rnd_search_cv.best_params_, -rnd_search_cv.best_score_))\n",
    "\n",
    "trainMSE = model.evaluate(Xtrain_full, Ytrain_full, verbose = verbosity)\n",
    "print(\"Train Score: {0:.5f} MSE {1:.5f} RMSE\".format(trainMSE, np.sqrt(trainMSE)))\n",
    "testMSE = model.evaluate(Xtest, Ytest, verbose = verbosity)\n",
    "print(\"Test Score: {0:.5f} MSE {1:.5f} RMSE\".format(testMSE, np.sqrt(testMSE)))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
