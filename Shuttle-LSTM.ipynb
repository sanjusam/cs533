{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\n",
      "tf version 2.2.0 keras version 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print('python version', sys.version_info)\n",
    "print('tf version', tf.__version__, 'keras version', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get look back dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input expected to be a 2D array with last column being label\n",
    "# Returns looked back X adn Y; last column in look back Y data returned is label\n",
    "# Only one step ahead prediction setting is expected.\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_seq1 = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150])\n",
    "in_seq2 = np.array([15, 25, 35, 45, 55, 65, 75, 85, 95, 105, 115, 125, 135, 145, 155])\n",
    "out_seq = np.array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "labels = np.array([0,1,0,1,0,1,0,1,0,1,0,1,0,1,0]).reshape((-1, 1))\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "mvdataWithLabels = np.hstack((in_seq1, in_seq2, out_seq, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 4),\n",
       " array([[ 10,  15,  25,   0],\n",
       "        [ 20,  25,  45,   1],\n",
       "        [ 30,  35,  65,   0],\n",
       "        [ 40,  45,  85,   1],\n",
       "        [ 50,  55, 105,   0],\n",
       "        [ 60,  65, 125,   1],\n",
       "        [ 70,  75, 145,   0],\n",
       "        [ 80,  85, 165,   1],\n",
       "        [ 90,  95, 185,   0],\n",
       "        [100, 105, 205,   1],\n",
       "        [110, 115, 225,   0],\n",
       "        [120, 125, 245,   1],\n",
       "        [130, 135, 265,   0],\n",
       "        [140, 145, 285,   1],\n",
       "        [150, 155, 305,   0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvdataWithLabels.shape, mvdataWithLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look back data shapes: lookbackX (10, 5, 3) lookbackY (10, 4)\n",
      "Look back lookBackTsData ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookbackX, lookbackY = look_back_and_create_dataset(mvdataWithLabels, look_back=5)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "print(\"Look back lookBackTsData ... \\n\")\n",
    "# for i in range (len(lookbackX)):\n",
    "#     print(lookbackX[i], lookbackX[i].shape, lookbackY[i], lookbackY[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 5, 3), (10, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = lookbackX\n",
    "Y = lookbackY[:, :-1] # exclude label\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]] (5, 3) [ 60  65 125] (3,)\n",
      "[[ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]] (5, 3) [ 70  75 145] (3,)\n",
      "[[ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]] (5, 3) [ 80  85 165] (3,)\n",
      "[[ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]] (5, 3) [ 90  95 185] (3,)\n",
      "[[ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]] (5, 3) [100 105 205] (3,)\n",
      "[[ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]\n",
      " [100 105 205]] (5, 3) [110 115 225] (3,)\n",
      "[[ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]\n",
      " [100 105 205]\n",
      " [110 115 225]] (5, 3) [120 125 245] (3,)\n",
      "[[ 80  85 165]\n",
      " [ 90  95 185]\n",
      " [100 105 205]\n",
      " [110 115 225]\n",
      " [120 125 245]] (5, 3) [130 135 265] (3,)\n",
      "[[ 90  95 185]\n",
      " [100 105 205]\n",
      " [110 115 225]\n",
      " [120 125 245]\n",
      " [130 135 265]] (5, 3) [140 145 285] (3,)\n",
      "[[100 105 205]\n",
      " [110 115 225]\n",
      " [120 125 245]\n",
      " [130 135 265]\n",
      " [140 145 285]] (5, 3) [150 155 305] (3,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    print(X[i], X[i].shape, Y[i], Y[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Statlog Shuttle multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Shuttle-LSTM.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Shuttle-LSTM.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def label_outliers(nasa_df_row):\n",
    "    if nasa_df_row['class'] == 1 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "    \n",
    "def cleanup() :\n",
    "    colnames =['time', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'class']\n",
    "    train_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.trn/shuttle.trn\",names=colnames,sep=\" \")\n",
    "    test_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.tst\",names=colnames,sep=\" \")\n",
    "\n",
    "    # merge train and test\n",
    "    merged_df = pd.concat([train_df, test_df])\n",
    "    # print(\"Unique classes {}\".format(np.unique(merged_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # drop class = 4\n",
    "    minus4_df = merged_df.loc[merged_df['class'] != 4]\n",
    "    # print(\"Frame after dropping 4 \\n{}\".format(minus4_df))\n",
    "    # print(\"Unique classes after dropping 4 {}\".format(np.unique(minus4_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # mark class 1 as inlier and rest as outlier\n",
    "    is_anomaly_column = minus4_df.apply(lambda row: label_outliers(row), axis=1)\n",
    "    labelled_df = minus4_df.assign(is_anomaly=is_anomaly_column.values)\n",
    "\n",
    "    #print(\"Frame after labelling outliers \\n{}\".format(labelled_df))\n",
    "    print(\"Unique classes after labelling outliers {}\".format(np.unique(labelled_df['class'].values, return_counts=True)))\n",
    "    print(\"Unique outliers after labelling outliers {}\".format(np.unique(labelled_df['is_anomaly'].values, return_counts=True)))\n",
    "\n",
    "    # sort by time\n",
    "\n",
    "    sorted_df = labelled_df.sort_values('time')\n",
    "\n",
    "    #print(\"Sorted Frame\\n{}\".format(sorted_df))\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "def read_data_with_labels(df, timeVariantColumns, labelColumnNum):\n",
    "#     df = pd.read_csv(file)\n",
    "    data = df.values.astype('float64')\n",
    "    tsData = df[timeVariantColumns].values.astype('float64')\n",
    "    labels = data[:, labelColumnNum].reshape((-1,1))\n",
    "    tsDataWithLabels = np.hstack((tsData, labels))\n",
    "    return tsDataWithLabels, data\n",
    "\n",
    "def scale(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(data)\n",
    "    return scaler, scaler.transform(data)\n",
    "\n",
    "\"\"\"\n",
    "# input expected to be a 2D array with last column being label\n",
    "# Returns looked back X adn Y; last column in look back Y data returned is label\n",
    "# Only one step ahead prediction setting is expected.\n",
    "\"\"\"\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)\n",
    "\n",
    "def split_data_set(dataset, split=0.67):\n",
    "    train_size = int(len(dataset) * split)\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    return train, test\n",
    "\n",
    "def get_train_validation(Xtrain, Ytrain, validation_ratio=0.1):\n",
    "    validation_size = int(len(Xtrain) * validation_ratio)\n",
    "    Xtrain, Xvalid = Xtrain[validation_size:], Xtrain[:validation_size]\n",
    "    Ytrain, Yvalid = Ytrain[validation_size:], Ytrain[:validation_size]\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid\n",
    "\n",
    "# Note here the slight change in how we stack the hidden LSTM layers - special for the last LSTM layer.\n",
    "def baseline_model(input_shape, learning_rate):\n",
    "    def build_model(input_shape=input_shape, n_hidden = 1, n_units = 50, learning_rate = learning_rate):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        for layer in range(n_hidden - 1):\n",
    "            # return sequence = true for all layers except last layer\n",
    "            model.add(keras.layers.LSTM(n_units, return_sequences = True, activation = 'relu'))\n",
    "        model.add(keras.layers.LSTM(n_units, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "    \n",
    "\n",
    "############## main #########################\n",
    "\n",
    "sorted_df = cleanup()\n",
    "\n",
    "timeVariantColumns = ['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8']\n",
    "labelColumnNum = 10\n",
    "\n",
    "# read data\n",
    "tsDataWithLabels, data = read_data_with_labels(sorted_df, timeVariantColumns, labelColumnNum)\n",
    "print(\"Shapes: time variant data array with labels {}, full data {}\".format(tsDataWithLabels.shape, data.shape))\n",
    "print(\"Unique outliers in full data array {}\".format(np.unique(data[:, -1], return_counts=True)))\n",
    "print(\"Unique outliers in time variant data array with labels {}\".format(np.unique(tsDataWithLabels[:, -1], \n",
    "                                                                                   return_counts=True)))\n",
    "\n",
    "# print(tsDataWithLabels)\n",
    "\n",
    "# scale data\n",
    "scaler, tsDataScaled = scale(tsDataWithLabels)\n",
    "\n",
    "look_back=24\n",
    "# Get look back data in the 3D array shape (n_samples, n_lookback_steps, n_features)\n",
    "lookbackX, lookbackY = look_back_and_create_dataset(tsDataScaled, look_back=look_back)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes after labelling outliers (array([1, 2, 3, 5, 6, 7]), array([45586,    50,   171,  3267,    10,    13]))\n",
      "Unique outliers after labelling outliers (array([0, 1]), array([45586,  3511]))\n",
      "Shapes: time variant data array with labels (49097, 9), full data (49097, 11)\n",
      "Unique outliers in full data array (array([0., 1.]), array([45586,  3511]))\n",
      "Unique outliers in time variant data array with labels (array([0., 1.]), array([45586,  3511]))\n",
      "Look back data shapes: lookbackX (49073, 24, 8) lookbackY (49073, 9)\n"
     ]
    }
   ],
   "source": [
    "# %load Shuttle-LSTM.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def label_outliers(nasa_df_row):\n",
    "    if nasa_df_row['class'] == 1 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "    \n",
    "def cleanup() :\n",
    "    colnames =['time', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'class']\n",
    "    train_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.trn/shuttle.trn\",names=colnames,sep=\" \")\n",
    "    test_df = pd.read_csv(\"../Stochastic-Methods/data/nasa/shuttle.tst\",names=colnames,sep=\" \")\n",
    "\n",
    "    # merge train and test\n",
    "    merged_df = pd.concat([train_df, test_df])\n",
    "    # print(\"Unique classes {}\".format(np.unique(merged_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # drop class = 4\n",
    "    minus4_df = merged_df.loc[merged_df['class'] != 4]\n",
    "    # print(\"Frame after dropping 4 \\n{}\".format(minus4_df))\n",
    "    # print(\"Unique classes after dropping 4 {}\".format(np.unique(minus4_df['class'].values, return_counts=True)))\n",
    "\n",
    "    # mark class 1 as inlier and rest as outlier\n",
    "    is_anomaly_column = minus4_df.apply(lambda row: label_outliers(row), axis=1)\n",
    "    labelled_df = minus4_df.assign(is_anomaly=is_anomaly_column.values)\n",
    "\n",
    "    #print(\"Frame after labelling outliers \\n{}\".format(labelled_df))\n",
    "    print(\"Unique classes after labelling outliers {}\".format(np.unique(labelled_df['class'].values, return_counts=True)))\n",
    "    print(\"Unique outliers after labelling outliers {}\".format(np.unique(labelled_df['is_anomaly'].values, return_counts=True)))\n",
    "\n",
    "    # sort by time\n",
    "\n",
    "    sorted_df = labelled_df.sort_values('time')\n",
    "\n",
    "    #print(\"Sorted Frame\\n{}\".format(sorted_df))\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "def read_data_with_labels(df, timeVariantColumns, labelColumnNum):\n",
    "#     df = pd.read_csv(file)\n",
    "    data = df.values.astype('float64')\n",
    "    tsData = df[timeVariantColumns].values.astype('float64')\n",
    "    labels = data[:, labelColumnNum].reshape((-1,1))\n",
    "    tsDataWithLabels = np.hstack((tsData, labels))\n",
    "    return tsDataWithLabels, data\n",
    "\n",
    "def scale(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(data)\n",
    "    return scaler, scaler.transform(data)\n",
    "\n",
    "\"\"\"\n",
    "# input expected to be a 2D array with last column being label\n",
    "# Returns looked back X adn Y; last column in look back Y data returned is label\n",
    "# Only one step ahead prediction setting is expected.\n",
    "\"\"\"\n",
    "\n",
    "def look_back_and_create_dataset(tsDataWithLabels, look_back = 1):\n",
    "    lookbackTsDataX = [] \n",
    "    lookbackTsDataYAndLabel = []\n",
    "    for i in range(look_back, len(tsDataWithLabels)):\n",
    "        a = tsDataWithLabels[i-look_back:i, :-1]\n",
    "        lookbackTsDataX.append(a)\n",
    "        lookbackTsDataYAndLabel.append(tsDataWithLabels[i])\n",
    "    return np.array(lookbackTsDataX), np.array(lookbackTsDataYAndLabel)\n",
    "\n",
    "def split_data_set(dataset, split=0.67):\n",
    "    train_size = int(len(dataset) * split)\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    return train, test\n",
    "\n",
    "def get_train_validation(Xtrain, Ytrain, validation_ratio=0.1):\n",
    "    validation_size = int(len(Xtrain) * validation_ratio)\n",
    "    Xtrain, Xvalid = Xtrain[validation_size:], Xtrain[:validation_size]\n",
    "    Ytrain, Yvalid = Ytrain[validation_size:], Ytrain[:validation_size]\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid\n",
    "\n",
    "# Note here the slight change in how we stack the hidden LSTM layers - special for the last LSTM layer.\n",
    "def baseline_model(input_shape, learning_rate):\n",
    "    def build_model(input_shape=input_shape, n_hidden = 1, n_units = 50, learning_rate = learning_rate):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        for layer in range(n_hidden - 1):\n",
    "            # return sequence = true for all layers except last layer\n",
    "            model.add(keras.layers.LSTM(n_units, return_sequences = True, activation = 'relu'))\n",
    "        model.add(keras.layers.LSTM(n_units, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "    \n",
    "\n",
    "############## main #########################\n",
    "\n",
    "sorted_df = cleanup()\n",
    "\n",
    "timeVariantColumns = ['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8']\n",
    "labelColumnNum = 10\n",
    "\n",
    "# read data\n",
    "tsDataWithLabels, data = read_data_with_labels(sorted_df, timeVariantColumns, labelColumnNum)\n",
    "print(\"Shapes: time variant data array with labels {}, full data {}\".format(tsDataWithLabels.shape, data.shape))\n",
    "print(\"Unique outliers in full data array {}\".format(np.unique(data[:, -1], return_counts=True)))\n",
    "print(\"Unique outliers in time variant data array with labels {}\".format(np.unique(tsDataWithLabels[:, -1], \n",
    "                                                                                   return_counts=True)))\n",
    "\n",
    "# print(tsDataWithLabels)\n",
    "\n",
    "# scale data\n",
    "scaler, tsDataScaled = scale(tsDataWithLabels)\n",
    "\n",
    "look_back=24\n",
    "# Get look back data in the 3D array shape (n_samples, n_lookback_steps, n_features)\n",
    "lookbackX, lookbackY = look_back_and_create_dataset(tsDataScaled, look_back=look_back)\n",
    "print(\"Look back data shapes: lookbackX {} lookbackY {}\".format(lookbackX.shape, lookbackY.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
